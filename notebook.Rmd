---
title: "BDA-project"
output: pdf_document
---

# Loaded packages

```{r, echo=TRUE}
library(tidyr)
library(dplyr)
library(rstan)
library(rstanarm)
library(bayesplot)
library(aaltobda)
options(mc.cores = parallel::detectCores())
library(loo)
library(ggplot2)
library(gridExtra)
library(bayesplot)
```

# Introduction

// this part has to be inviting xD
// General talk about Finalnd having high rates of domestic violence, need to raise awareness, etc.

// Main topic/purpose of the project/notebook.

// Present outline of the notebook 

// structure and organization has to be logical

## Data description

Here is a tabular representation of the data.

```{r}
data = read.csv("dataset.csv", header = TRUE, sep=";")
data
```

```{r}
source('stan_utility.R')
lsf.str()
```

Below are scatter plots for each of the age groups

```{r}
ageGroups <- unique(data$Victim.s.age)
print(ageGroups)
grouped_data <- c()

data_for_age_group <- filter(data, Victim.s.age == ageGroups[1])
ggplot(aes(Year, Number.of.victims), data = data_for_age_group) + geom_point(size = 1)  + geom_smooth()  

data_for_age_group <- filter(data, Victim.s.age == ageGroups[2])
ggplot(aes(Year, Number.of.victims), data = data_for_age_group) + geom_point(size = 1)  + geom_smooth()  

data_for_age_group <- filter(data, Victim.s.age == ageGroups[3])
ggplot(aes(Year, Number.of.victims), data = data_for_age_group) + geom_point(size = 1)  + geom_smooth()  

data_for_age_group <- filter(data, Victim.s.age == ageGroups[4])
ggplot(aes(Year, Number.of.victims), data = data_for_age_group) + geom_point(size = 1)  + geom_smooth()  

data_for_age_group <- filter(data, Victim.s.age == ageGroups[5])
ggplot(aes(Year, Number.of.victims), data = data_for_age_group) + geom_point(size = 1)  + geom_smooth()  

data_for_age_group <- filter(data, Victim.s.age == ageGroups[6])
ggplot(aes(Year, Number.of.victims), data = data_for_age_group) + geom_point(size = 1)  + geom_smooth()  

data_for_age_group <- filter(data, Victim.s.age == ageGroups[7])
ggplot(aes(Year, Number.of.victims), data = data_for_age_group) + geom_point(size = 1)  + geom_smooth()  

data_for_age_group <- filter(data, Victim.s.age == ageGroups[8])
ggplot(aes(Year, Number.of.victims), data = data_for_age_group) + geom_point(size = 1)  + geom_smooth()  

data_for_age_group <- filter(data, Victim.s.age == ageGroups[9])
ggplot(aes(Year, Number.of.victims), data = data_for_age_group) + geom_point(size = 1)  + geom_smooth()  

data_for_age_group <- filter(data, Victim.s.age == ageGroups[10])
ggplot(aes(Year, Number.of.victims), data = data_for_age_group) + geom_point(size = 1)  + geom_smooth()  

data_for_age_group <- filter(data, Victim.s.age == ageGroups[11])
ggplot(aes(Year, Number.of.victims), data = data_for_age_group) + geom_point(size = 1)  + geom_smooth()  

```

## Analysis problem

## Models

```{r}
SEED <- 1474652
```

// add a clear list of models

### Model 1

*The centered parameterization of our hierarchical model.*

```{stan, output.var="stan_model_ar_hierarchical"}
data {
  int<lower=0> N;
  int<lower=0> K; // number of groups
  int<lower=1,upper=K> x[N]; // group indicator
  vector[N] y;
}
parameters {
  real mu_group_alpha;
  real mu_group_beta;
  real<lower=0> tau_alpha; // prior std
  real<lower=0> tau_beta; // prior std
  vector[K] alpha;
  vector[K] beta;
  real<lower=0> sigma;
}
transformed parameters {
  real<lower=0> mu[N-K];
  
  for (n in K+1:N) {
    mu[n-K] = alpha[x[n]] + beta[x[n]] * y[n-K];
  }
}
model {
  mu_group_alpha ~ normal(0, 100000); // weakly informative prior
  mu_group_beta ~ normal(1, 1000); // weakly informative prior
  tau_alpha ~ cauchy(0, 5000); // weakly informative prior
  tau_beta ~ cauchy(0, 5000); // weakly informative prior
  sigma ~ cauchy(0, 5000); // weakly informative prior
  
  alpha ~ normal(mu_group_alpha, tau_alpha);
  beta ~ normal(mu_group_beta, tau_beta);
  for (n in K+1:N) {
     y[n] ~ normal(mu[n-K], sigma);
  }
}
generated quantities {
  vector[N-K] log_lik;
  for (i in K+1:N)
    log_lik[i-K] = normal_lpdf(y[i] | mu[i-K], sigma);
}
```

Preparing the data

```{r}
hierarchical_data <- sapply(2009:2018, function (j) sapply(1:11, function (i) filter(data, Victim.s.age == ageGroups[i] & Year == j)$Number.of.victims))
hierarchical_data
```

Fitting the model

```{r}
nrows <- nrow(hierarchical_data)
stan_data = list(
  N = (2018-2009+1) * 11,
  K = 11,
  x = rep(1:nrow(hierarchical_data), ncol(hierarchical_data)),
  y = c(hierarchical_data)
)
fit_hierarchical = sampling(stan_model_ar_hierarchical, SEED <- 1141, data = stan_data, seed = SEED, iter = 10000)
print(fit_hierarchical)
```

Check tree depth

```{r}
check_treedepth(fit_hierarchical)
```

Check the energy Bayesian Fraction of Missing Information

```{r}
rstan::check_energy(fit_hierarchical)
```

```{r}
check_div(fit_hierarchical)
```

Run the posterior predictive checking on the fit

```{r}
log_lik_hierarchical <- extract_log_lik(fit_hierarchical, merge_chains = FALSE)

r_eff_hierarchical <- relative_eff(exp(log_lik_hierarchical)) 
loo_hierarchical<- loo(log_lik_hierarchical, r_eff = r_eff_hierarchical)
print(loo_hierarchical)
plot(loo_hierarchical)
```

Although the posterior predictive chcking looks good, the fit had 1228 divergent transitions, and ALL 4 chains indicated pathological behaviour accordinng to E-BFMI.

The model certainly has some issues, so let's try instead  the complementary non-centered parameterization (the idea to use it to improve the issues mentioned above was adapted from here https://mc-stan.org/users/documentation/case-studies/rstan_workflow.html)


```{stan, output.var="stan_model_ar_hierarchical"}
data {
  int<lower=0> N;
  int<lower=0> K; // number of groups
  int<lower=1,upper=K> x[N]; // group indicator
  int<lower=0> y[N];
  int<lower=0> y0[K];
}
parameters {
  real mu_group_alpha;
  real mu_group_beta;
  real<lower=0> tau_alpha; // prior std
  real<lower=0> tau_beta; // prior std
  vector[K] alpha_tilde;
  vector[K] beta_tilde;
}
transformed parameters {
  real<lower=0> mu[N];
  real alpha[K];
  real beta[K];
  
  for (i in 1:K) {
    alpha[i] = mu_group_alpha + tau_alpha * alpha_tilde[i];
    beta[i] = mu_group_beta + tau_beta * beta_tilde[i];
  }
  
  for (n in 1:K) {
    mu[n] = alpha[x[n]] + beta[x[n]] * y0[n];
  }
  for (n in K+1:N) {
    mu[n] = alpha[x[n]] + beta[x[n]] * y[n-K];
  }
}
model {
  mu_group_alpha ~ normal(0, 100000); // weakly informative prior
  mu_group_beta ~ normal(0, 1000); // weakly informative prior
  tau_alpha ~ cauchy(0, 5000); // weakly informative prior
  tau_beta ~ cauchy(0, 5000); // weakly informative prior
  
  alpha_tilde ~ normal(0, 1);
  beta_tilde ~ normal(0, 1);
  for (n in 1:N) {
     y[n] ~ poisson(mu[n]);
  }
}
generated quantities {
  vector[N] log_lik;
  vector[N] y_rep;
  
  for (i in 1:N) {
    y_rep[i] = poisson_rng(mu[i]);
    log_lik[i] = poisson_lpmf(y[i] | mu[i]);
  }
}
```



```{r}
nrows <- nrow(hierarchical_data)
stan_data = list(
  N = (2018-2010+1) * 11,
  K = 11,
  x = rep(1:nrow(hierarchical_data), 9),
  y = tail(c(hierarchical_data), -11),
  y0 = head(c(hierarchical_data), 11)
)
fit_hierarchical = sampling(stan_model_ar_hierarchical,data = stan_data, seed = 12452, iter = 5000, control=list(max_treedepth=15))
print(fit_hierarchical)
```

```{r}
check_treedepth(fit_hierarchical, 15)
```

```{r}
rstan::check_energy(fit_hierarchical)
```

```{r}
check_div(fit_hierarchical)
```

```{r}
indexes <- seq(1, 99, 11)
samples_poisson <- rstan::extract(fit_hierarchical, permuted = T)
mu <- apply(samples_poisson$mu, 2, quantile, c(0.05, 0.5, 0.95))
mu_means <- mu[2,]
plot(mu_means[indexes])
# plot(mu_means)
```

```{r}
# length(y)
plot(y[indexes])
```


*PSIS-LOO on the whole dataset*


```{r}
# log_lik_hierarchical_all <- extract_log_lik(fit_hierarchical, merge_chains = FALSE)
dim(fit_hierarchical)
```



```{r}
N <- 99
L <- 11 * 3 # fit_hierarchical does not have the first year already, so we only need to remove 3 years from teh beginning
# indexes <- seq(2, 99, 11)

log_lik_hierarchical_all <- extract_log_lik(fit_hierarchical, merge_chains = FALSE)
log_lik_hierarchical <- log_lik_hierarchical_all[, , (L+1):N]

r_eff_hierarchical <- relative_eff(exp(log_lik_hierarchical)) 
loo_hierarchical<- loo(log_lik_hierarchical, r_eff = r_eff_hierarchical)
print(loo_hierarchical)
plot(loo_hierarchical)
```

```{stan, output.var="stan_model_ar_hierarchical_cv"}
data {
  int<lower=0> N;
  int<lower=0> K; // number of groups
  int<lower=1,upper=K> x[N]; // group indicator
  int<lower=0> y[N];
  int<lower=0> y0[K];
}
parameters {
  real mu_group_alpha;
  real mu_group_beta;
  real<lower=0> tau_alpha; // prior std
  real<lower=0> tau_beta; // prior std
  vector[K] alpha_tilde;
  vector[K] beta_tilde;
}
transformed parameters {
  real<lower=0> mu[N-K];
  real alpha[K];
  real beta[K];
  
  for (i in 1:K) {
    alpha[i] = mu_group_alpha + tau_alpha * alpha_tilde[i];
    beta[i] = mu_group_beta + tau_beta * beta_tilde[i];
  }
  
  for (n in 1:K) {
    mu[n] = alpha[x[n]] + beta[x[n]] * y0[n];
  }
  for (n in K+1:(N - (1 * K))) {
    mu[n] = alpha[x[n]] + beta[x[n]] * y[n-K];
  }
}
model {
  mu_group_alpha ~ normal(0, 100000); // weakly informative prior
  mu_group_beta ~ normal(0, 1000); // weakly informative prior
  tau_alpha ~ cauchy(0, 5000); // weakly informative prior
  tau_beta ~ cauchy(0, 5000); // weakly informative prior
  
  alpha_tilde ~ normal(0, 1);
  beta_tilde ~ normal(0, 1);
  for (n in 1:(N - (1 * K))) {
     y[n] ~ poisson(mu[n]);
  }
}
  
generated quantities {
  vector[N] log_lik;
  real<lower=0> mu_pred[K];
  
  for (n in 1:K) {
    mu_pred[n] = alpha[n] + beta[n] * y[(N - (1 * K)) - K + n];
  }
  
  for (i in 1:(N - (1 * K))) {
    log_lik[i] = poisson_lpmf(y[i] | mu[i]);
  }
  
  for (i in 1:K) {
    log_lik[(N - (1 * K)) + i] = poisson_lpmf(y[(N - (1 * K)) + i] | mu_pred[i]);
  }
}
```

The code below is adapted from http://mc-stan.org/loo/articles/loo2-lfo.html


```{r}
N = 10
L = 4
K = 11

loglik_exact <- matrix(nrow = 10000, ncol = N * K)

for (i in seq(N, L+1, -1)) {
  hierarchical_data_i <- hierarchical_data[, (1:i)]
  stan_data_i = list(
    N = (i-1) * 11,
    K = 11,
    x = rep(1:nrow(hierarchical_data_i), i - 1),
    y = tail(c(hierarchical_data_i), -11),
    y0 = head(c(hierarchical_data_i), 11)
  )
  fit_i = sampling(stan_model_ar_hierarchical_cv, data = stan_data_i, seed = 12452, refresh=0, iter = 5000, control=list(max_treedepth=15))
  last_K_values <- seq(((i-2) * K + 1), ((i - 1) * K))
  loglik_exact[, last_K_values] <- extract_log_lik(fit_i)[, last_K_values]
}
```

```{r}
loglik_exact[1, 34:99]
```


```{r}
# some helper functions we'll use throughout

# more stable than log(sum(exp(x))) 
log_sum_exp <- function(x) {
  max_x <- max(x)  
  max_x + log(sum(exp(x - max_x)))
}

# more stable than log(mean(exp(x)))
log_mean_exp <- function(x) {
  log_sum_exp(x) - log(length(x))
}

# compute log of raw importance ratios
# sums over observations *not* over posterior samples
sum_log_ratios <- function(ll, ids = NULL) {
  if (!is.null(ids)) ll <- ll[, ids, drop = FALSE]
  - rowSums(ll)
}

# for printing comparisons later
rbind_print <- function(...) {
  round(rbind(...), digits = 2)
}
```

```{r}
exact_elpds_1sap <- apply(loglik_exact, 2, log_mean_exp)
print(exact_elpds_1sap)

```

```{r}
exact_elpd_1sap <- c(ELPD = sum(exact_elpds_1sap[34:99]))

rbind_print(
  "LFO" = exact_elpd_1sap
)
```

### Graphical posterior predictive checking

```{r}
y_rep <- as.matrix(fit_hierarchical, pars = "y_rep")
dim(y_rep)
```

```{r}
group_idx <- 1
indexes <- seq(group_idx, 99, 11)
ppc_ribbon(c(hierarchical_data[group_idx, (2:10)]), y_rep[, indexes]) 

group_idx <- 2
indexes <- seq(group_idx, 99, 11)
ppc_ribbon(c(hierarchical_data[group_idx, (2:10)]), y_rep[, indexes]) 

group_idx <- 3
indexes <- seq(group_idx, 99, 11)
ppc_ribbon(c(hierarchical_data[group_idx, (2:10)]), y_rep[, indexes]) 

group_idx <- 4
indexes <- seq(group_idx, 99, 11)
ppc_ribbon(c(hierarchical_data[group_idx, (2:10)]), y_rep[, indexes]) 

group_idx <- 5
indexes <- seq(group_idx, 99, 11)
ppc_ribbon(c(hierarchical_data[group_idx, (2:10)]), y_rep[, indexes]) 

group_idx <- 6
indexes <- seq(group_idx, 99, 11)
ppc_ribbon(c(hierarchical_data[group_idx, (2:10)]), y_rep[, indexes]) 

group_idx <- 7
indexes <- seq(group_idx, 99, 11)
ppc_ribbon(c(hierarchical_data[group_idx, (2:10)]), y_rep[, indexes]) 

group_idx <- 8
indexes <- seq(group_idx, 99, 11)
ppc_ribbon(c(hierarchical_data[group_idx, (2:10)]), y_rep[, indexes]) 

group_idx <- 9
indexes <- seq(group_idx, 99, 11)
ppc_ribbon(c(hierarchical_data[group_idx, (2:10)]), y_rep[, indexes]) 

group_idx <- 10
indexes <- seq(group_idx, 99, 11)
ppc_ribbon(c(hierarchical_data[group_idx, (2:10)]), y_rep[, indexes])

group_idx <- 11
indexes <- seq(group_idx, 99, 11)
ppc_ribbon(c(hierarchical_data[group_idx, (2:10)]), y_rep[, indexes]) 
```


Thus, we figured out that the problem is caused by age group 7 and 8. 

Now let's try another sensible but a little stronger informative prior, while keeping results of the previous fit as our base-line.

```{stan, output.var="stan_model_ar_hierarchical"}
data {
  int<lower=0> N;
  int<lower=0> K; // number of groups
  int<lower=1,upper=K> x[N]; // group indicator
  vector[N] y;
}
parameters {
  real mu_group_alpha;
  real mu_group_beta;
  real<lower=0> tau_alpha; // prior std
  real<lower=0> tau_beta; // prior std
  vector[K] alpha_tilde;
  vector[K] beta_tilde;
  real<lower=0> sigma;
}
transformed parameters {
  vector [N-K] mu;
  vector[K] alpha;
  vector[K] beta;
  
  for (i in 1:K) {
    alpha[i] = mu_group_alpha + tau_alpha * alpha_tilde[i];
    beta[i] = mu_group_beta + tau_beta * beta_tilde[i];
  }
  
  for (n in K+1:N) {
    mu[n-K] = alpha[x[n]] + beta[x[n]] * y[n-K];
  }
}
model {
  mu_group_alpha ~ normal(0, 5000); // a bit stronger informative prior
  mu_group_beta ~ normal(0, 1); //a bit stronger informative prior
  tau_alpha ~ cauchy(0, 1000); //a bit stronger informative prior
  tau_beta ~ cauchy(0, 1); // a bit stronger informative prior
  sigma ~ cauchy(0, 1000); // a bit stronger informative prior
  
  alpha_tilde ~ normal(0, 1);
  beta_tilde ~ normal(0, 1);
  for (n in K+1:N) {
     y[n] ~ normal(mu[n-K], sigma);
  }
}
generated quantities {
  vector[N-K] log_lik;
  vector[N-K] y_rep;
  for (i in K+1:N) {
    y_rep[i-K] = normal_rng(mu[i-K], sigma);
    log_lik[i-K] = normal_lpdf(y[i] | mu[i-K], sigma);
  }
}
```

```{r}
nrows <- nrow(hierarchical_data)
stan_data = list(
  N = (2018-2009+1) * 11,
  K = 11,
  x = rep(1:nrow(hierarchical_data), ncol(hierarchical_data)),
  y = c(hierarchical_data)
)
fit_hierarchical = sampling(stan_model_ar_hierarchical, data = stan_data, seed = SEED, iter = 10000, control=list(adapt_delta=0.90))
print(fit_hierarchical)
```

```{r}
check_treedepth(fit_hierarchical)
```

```{r}
rstan::check_energy(fit_hierarchical)
```

```{r}
check_div(fit_hierarchical)
```

```{r}
c_dark <- c("#8F272780")
green <- c("#00FF0080")

partition <- partition_div(fit_hierarchical)
div_params <- partition[[1]]
nondiv_params <- partition[[2]]

par(mar = c(4, 4, 0.5, 0.5))
plot(nondiv_params$'beta[8]', log(nondiv_params$tau_alpha),
     col=c_dark, pch=16, cex=0.8, xlab="beta[8]", ylab="log(tau_beta)",
     )
points(div_params$'beta[8]', log(div_params$tau_alpha),
       col=green, pch=16, cex=0.8)
```

```{r}
y_rep <- as.matrix(fit_hierarchical, pars = "y_rep")

# number of rows = number of post-warmup posterior draws
# number of columns = length(y)
dim(y_rep) 
```

```{r}
y <- tail(c(hierarchical_data), -11)
```


```{r}
indexes <- seq(1, length(y), 11)
ppc_dens_overlay(y[indexes], y_rep[1:4, indexes])
```

*PSIS-LOO on the whole dataset*

```{r}
log_lik_hierarchical <- extract_log_lik(fit_hierarchical, merge_chains = FALSE)

r_eff_hierarchical <- relative_eff(exp(log_lik_hierarchical)) 
loo_hierarchical<- loo(log_lik_hierarchical, r_eff = r_eff_hierarchical)
print(loo_hierarchical)
plot(loo_hierarchical)
```


// include proper prior, jusify

// Rhat convergence diagnostics

// HMC specific convergence diagnostics (divergences, tree depth)

// ESS diagnostics

// posterior predictive checking

### Model 2

```{stan, output.var="stan_model_ar_pooled"}
data {
  int<lower=0> N;
  vector[N] y;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}
transformed parameters {
  vector [N-1] mu;
  
  for (n in 2:N) {
    mu[n-1] = alpha + beta * y[n-1];
  }
}
model {
  for (n in 2:N) {
     y[n] ~ normal(mu[n-1], sigma);
  }
}
generated quantities {
  vector[N-1] log_lik;
  for (i in 2:N)
    log_lik[i-1] = normal_lpdf(y[i] | mu[i-1], sigma);
}
```

```{r}
pooled_data <- sapply(2009:2018, function (i) sum(filter(data, Year == i)$Number.of.victims))
pooled_data
```

```{r}

stan_data = list(
  N = 2018-2009+1,
  y = pooled_data
)
fit_pooled = sampling(stan_model_ar_pooled, data = stan_data, iter = 10000)
print(fit_pooled)
```



```{r}
samples_lin <- rstan::extract(fit_pooled, permuted = T)
```

```{r}
samples_lin <- rstan::extract(fit_pooled, permuted = T)
mu <- apply(samples_lin$mu, 2, quantile, c(0.05, 0.5, 0.95)) %>%
t() %>% data.frame(x = seq(2010,2018), .) %>% gather(pct, y, -x)
stan_data = list(x = seq(2010,2018), y = stan_data$y)
pfit <- ggplot() +
geom_point(aes(x, y), data = data.frame(stan_data), size = 1) + geom_line(aes(x, y, linetype = pct), data = mu, color = 'red') + scale_linetype_manual(values = c(2,1,2)) +
labs(y = 'Summer temp. @Kilpisjärvi', x= "Year") +
guides(linetype = F)
pfit
```

```{r}
log_lik_pooled <- extract_log_lik(fit_pooled, merge_chains = FALSE)

r_eff_pooled <- relative_eff(exp(log_lik_pooled)) 
loo_pooled <- loo(log_lik_pooled, r_eff = r_eff_pooled)
print(loo_pooled)
plot(loo_pooled)
```

Let's make our priors more informative

```{stan, output.var="stan_model_ar_hierarchical"}
data {
  int<lower=0> N;
  int<lower=0> K; // number of groups
  int<lower=1,upper=K> x[N]; // group indicator
  vector[N] y;
}
parameters {
  real mu_group_alpha;
  real mu_group_beta;
  real<lower=0> tau_alpha; // prior std
  real<lower=0> tau_beta; // prior std
  vector[K] alpha_tilde;
  vector[K] beta_tilde;
  real<lower=0> sigma;
}
transformed parameters {
  vector [N-K] mu;
  vector[K] alpha;
  vector[K] beta;
  
  for (i in 1:K) {
    alpha[i] = mu_group_alpha + tau_alpha * alpha_tilde[i];
    beta[i] = mu_group_beta + tau_beta * beta_tilde[i];
  }
  
  for (n in K+1:N) {
    mu[n-K] = alpha[x[n]] + beta[x[n]] * y[n-K];
  }
}
model {
  mu_group_alpha ~ normal(0, 100000); // weakly informative prior
  mu_group_beta ~ normal(1, 1000); // weakly informative prior
  sigma ~ cauchy(0, 5000); // weakly informative prior
  
  alpha_tilde ~ normal(0, 1);
  beta_tilde ~ normal(0, 1);
  for (n in K+1:N) {
     y[n] ~ normal(mu[n-K], sigma);
  }
}
generated quantities {
  vector[N-K] log_lik;
  vector[N-K] y_rep;
  for (i in K+1:N) {
    y_rep[i-K] = normal_rng(mu[i-K], sigma);
    log_lik[i-K] = normal_lpdf(y[i] | mu[i-K], sigma);
  }
}
```

```{r}
nrows <- nrow(hierarchical_data)
stan_data = list(
  N = (2018-2009+1) * 11,
  K = 11,
  x = rep(1:nrow(hierarchical_data), ncol(hierarchical_data)),
  y = c(hierarchical_data)
)
fit_hierarchical = sampling(stan_model_ar_hierarchical, data = stan_data, seed = SEED, iter = 10000)
print(fit_hierarchical)
```

```{r}
check_treedepth(fit_hierarchical)
```

```{r}
rstan::check_energy(fit_hierarchical)
```

```{r}
check_div(fit_hierarchical)
```

```{r}
c_dark <- c("#8F272780")
green <- c("#00FF0080")

partition <- partition_div(fit_hierarchical)
div_params <- partition[[1]]
nondiv_params <- partition[[2]]

par(mar = c(4, 4, 0.5, 0.5))
plot(nondiv_params$'beta[3]', log(nondiv_params$tau_alpha),
     col=c_dark, pch=16, cex=0.8, xlab="beta[3]", ylab="log(tau_beta)",
     )
points(div_params$'beta[3]', log(div_params$tau_alpha),
       col=green, pch=16, cex=0.8)
```

```{r}
y_rep <- as.matrix(fit_hierarchical, pars = "y_rep")

# number of rows = number of post-warmup posterior draws
# number of columns = length(y)
dim(y_rep) 
```
```{r}
tail(c(t(hierarchical_data)), -11)
plot(t[1:10])
```

```{r}
y <- tail(c(hierarchical_data), -11)
```

```{r}
plot(y[indexes])
```


```{r}
indexes <- seq(1, length(y), 11)
ppc_dens_overlay(y[indexes], y_rep[1:4, indexes])
```


```{r}
log_lik_hierarchical <- extract_log_lik(fit_hierarchical, merge_chains = FALSE)

r_eff_hierarchical <- relative_eff(exp(log_lik_hierarchical)) 
loo_hierarchical<- loo(log_lik_hierarchical, r_eff = r_eff_hierarchical)
print(loo_hierarchical)
plot(loo_hierarchical)
```


The model seems to perform way better accorfing to all indications. 

// include proper prior, jusify

// Rhat convergence diagnostics

// HMC specific convergence diagnostics (divergences, tree depth)

// ESS diagnostics

// posterior predictive checking

### Model 2

```{stan, output.var="stan_model_ar_separate"}
data {
  int<lower=0> N;
  int<lower=0> K; // number of groups
  int<lower=1,upper=K> x[N]; // group indicator
  vector[N] y;
}
parameters {
  vector[K] alpha;
  vector[K] beta;
  real<lower=0> sigma;
}
transformed parameters {
  vector [N-K] mu;

  for (n in K+1:N) {
    mu[n-K] = alpha[x[n]] + beta[x[n]] * y[n-K];
  }
}
model {
  sigma ~ cauchy(0, 5000); // weakly informative prior
  alpha ~ normal(0, 10000);
  beta ~ normal(0, 100);
  
  for (n in K+1:N) {
     y[n] ~ normal(mu[n-K], sigma);
  }
}
generated quantities {
  vector[N-K] log_lik;
  vector[N-K] y_rep;
  for (i in K+1:N) {
    y_rep[i-K] = normal_rng(mu[i-K], sigma);
    log_lik[i-K] = normal_lpdf(y[i] | mu[i-K], sigma);
  }
}
```

```{r}
nrows <- nrow(hierarchical_data)
stan_data = list(
  N = (2018-2009+1) * 11,
  K = 11,
  x = rep(1:nrow(hierarchical_data), ncol(hierarchical_data)),
  y = c(hierarchical_data)
)
fit_separate = sampling(stan_model_ar_separate, data = stan_data, seed = SEED, iter = 10000)
print(fit_separate)
```

```{r}
check_treedepth(fit_separate)
```

```{r}
rstan::check_energy(fit_separate)
```

```{r}
check_div(fit_separate)
```

```{r}
c_dark <- c("#8F272780")
green <- c("#00FF0080")

partition <- partition_div(fit_separate)
div_params <- partition[[1]]
nondiv_params <- partition[[2]]

par(mar = c(4, 4, 0.5, 0.5))
plot(nondiv_params$'beta[3]', log(nondiv_params$tau_alpha),
     col=c_dark, pch=16, cex=0.8, xlab="beta[3]", ylab="log(tau_beta)",
     )
points(div_params$'beta[3]', log(div_params$tau_alpha),
       col=green, pch=16, cex=0.8)
```

```{r}
y_rep <- as.matrix(fit_separate, pars = "y_rep")

# number of rows = number of post-warmup posterior draws
# number of columns = length(y)
dim(y_rep) 
```

```{r}
y <- tail(c(hierarchical_data), -11)
```

```{r}
plot(y[indexes])
```


```{r}
indexes <- seq(1, length(y), 11)
ppc_dens_overlay(y[indexes], y_rep[1:4, indexes])
```


```{r}
log_lik_separate <- extract_log_lik(fit_separate, merge_chains = FALSE)

r_eff_separate <- relative_eff(exp(log_lik_separate)) 
loo_separate <- loo(log_lik_separate, r_eff = r_eff_separate)
print(loo_separate)
plot(loo_separate)
```


// include proper prior, jusify

// Rhat convergence diagnostics

// HMC specific convergence diagnostics (divergences, tree depth)

// ESS diagnostics

// posterior predictive checking

## Problems encountered and potential improvements


# Conclusion

// a clear conclusion here
