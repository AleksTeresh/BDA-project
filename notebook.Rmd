---
title: "BDA project"
author: "Anonymous"
output:
  pdf_document:
    toc: yes
    toc_depth: 1
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '1'
  word_document:
    toc: yes
    toc_depth: '1'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Loaded packages

```{r, echo=TRUE}
suppressPackageStartupMessages({
  library(tidyr)
  library(dplyr)
  library(knitr)
  library(rstan)
  library(rstanarm)
  library(bayesplot)
  library(aaltobda)
  options(mc.cores = parallel::detectCores())
  library(loo)
  library(ggplot2)
  library(gridExtra)
  library(bayesplot)
})
```

```{r}
SEED <- 1474652
set.seed(SEED)
```

# Utility functions

```{r}
partial_percentage_hist <- function(
  data,
  upper_limit
) {
  truncated_data = data[data < upper_limit]
  h <- hist(truncated_data, plot = FALSE)
  h$density <- h$counts / sum(h$counts) * 100
  plot(h, freq=F, ylab='Percentage', col = '#6eb5db')
}

check_prior_hierarchical <- function(
  full_grouped_data, 
  mu_group_alpha_sd,
  mu_group_beta_sd,
  tau_alpha_sd,
  tau_beta_sd
) {
  N = 5000 # number of (succesfull) samples to draw
  K = 11 # number of hierarchical groups
  D = 10 # number of datapoints (i.e. number of years)
  
  counter_1e7 = 0
  counter_1e6 = 0
  counter_1e5 = 0
  counter_1e4 = 0
  
  y_pred_prior = array(rep(1, N*K*D), dim=c(N, K, D-1)) # initiaalize an empty array to hold data
  
  for (j in 1:K) { # number of age groups
    mu_group_alpha = rnorm(N, 0, mu_group_alpha_sd) # based on previous studies
      
    for (q in 1:N) {
      mu_group_beta = rnorm(1, 0, mu_group_beta_sd) # based on previous studies
      
      while (mu_group_beta < 0) {
        mu_group_beta = rnorm(1, 0, mu_group_beta_sd) # based on previous studies
      }
      
      while (TRUE) {
        tau_alpha = rcauchy(1, 0, tau_alpha_sd)
        tau_beta = rcauchy(1, 0, tau_beta_sd)
        
        while (tau_alpha < 0 || tau_beta < 0) {
          tau_alpha = rcauchy(1, 0, tau_alpha_sd)
          tau_beta = rcauchy(1, 0, tau_beta_sd)
        }
        
        alpha = rnorm(1, mu_group_alpha[q], tau_alpha)
        beta = rnorm(1, mu_group_beta, tau_beta)
        
        if (beta >= 0) {
          passed = TRUE
          for (i in 2:D) { # starting with 2nd day becase AR(1) model is used
            mu = alpha + beta * full_grouped_data[j, i-1]
            if (mu < 0) {
              passed = FALSE
              break
            }
          }
          
          if (passed) {
            break
          }  
        }
      }
      
      for (i in 2:D) { # starting with 2nd day becase AR(1) model is used
        mu = alpha + beta * full_grouped_data[j, i-1]
        
        if (mu > 0) {
          y_pred = rpois(1, mu)
          
          if (y_pred > 1e7) {
            counter_1e7 = counter_1e7 + 1
          }
          if (y_pred > 1e6) {
            counter_1e6 = counter_1e6 + 1
          }
          if (y_pred > 1e5) {
            counter_1e5 = counter_1e5 + 1
          }
          if (y_pred > 1e4) {
            counter_1e4 = counter_1e4 + 1
          }
    
          y_pred_prior[q, j, i-1] <- y_pred
        }
      }
    }
  }
  
  print(counter_1e7 / (N * K * (D - 1)))
  print(counter_1e6 / (N * K * (D - 1)))
  print(counter_1e5 / (N * K * (D - 1)))
  print(counter_1e4 / (N * K * (D - 1))) 
  
  partial_percentage_hist(y_pred_prior, 1e6)
  partial_percentage_hist(y_pred_prior, 1e5)
  partial_percentage_hist(y_pred_prior, 1e4)
}

print_fit_diagnostic <- function(fit, tree_depth) {
  check_treedepth(fit, tree_depth)
  rstan::check_energy(fit)
  check_div(fit)
}

get_loo_diagnostics <- function(fit) {
  N <- 99
  L <- 11 * 3 # fit_hierarchical does not have the first year already, so we only need to remove 3 years from the beginning
  # indexes <- seq(2, 99, 11)
  
  log_lik_all <- extract_log_lik(fit, merge_chains = FALSE)
  log_lik <- log_lik_all[, , (L+1):N]
  
  r_eff <- relative_eff(exp(log_lik))
  loo(log_lik, r_eff = r_eff)
}

get_lfo_cv_results_hierarchical <- function(
  mu_group_alpha_prior_sd,
  mu_group_beta_prior_sd,
  tau_alpha_prior_sd,
  tau_beta_prior_sd
) {
  N = 10
  L = 4
  K = 11
  
  loglik_exact_hierarchical <- matrix(nrow = 10000, ncol = N * K)
  
  for (i in seq(N, L+1, -1)) {
    full_grouped_data_i <- full_grouped_data[, (1:i)]
    stan_data_i = list(
      N = (i-1) * 11,
      K = 11,
      x = rep(1:nrow(full_grouped_data_i), i - 1),
      y = tail(c(full_grouped_data_i), -11),
      y0 = head(c(full_grouped_data_i), 11),
      mu_group_alpha_prior_sd = mu_group_alpha_prior_sd,
      mu_group_beta_prior_sd = mu_group_beta_prior_sd,
      tau_alpha_prior_sd = tau_alpha_prior_sd,
      tau_beta_prior_sd = tau_beta_prior_sd,
      use_lfo_cv = 1
    )
    fit_i = sampling(
      stan_model_ar_hierarchical,
      data = stan_data_i,
      seed = 12452,
      refresh=0,
      iter = 5000,
      control=list(max_treedepth=15)
    )
    last_K_values <- seq(((i-2) * K + 1), ((i - 1) * K))
    loglik_exact_hierarchical[, last_K_values] <- extract_log_lik(fit_i)[, last_K_values]
  }
  
  loglik_exact_hierarchical
}

# more stable than log(sum(exp(x))) 
log_sum_exp <- function(x) {
  max_x <- max(x)  
  max_x + log(sum(exp(x - max_x)))
}

# more stable than log(mean(exp(x)))
log_mean_exp <- function(x) {
  log_sum_exp(x) - log(length(x))
}

# compute log of raw importance ratios
# sums over observations *not* over posterior samples
sum_log_ratios <- function(ll, ids = NULL) {
  if (!is.null(ids)) ll <- ll[, ids, drop = FALSE]
  - rowSums(ll)
}

# for printing comparisons later
rbind_print <- function(...) {
  round(rbind(...), digits = 2)
}

check_prior_separate <- function(
  full_grouped_data, 
  alpha_prior_mean,
  alpha_prior_sd,
  beta_prior_mean,
  beta_prior_sd
) {
  N = 3000 # number of (succesfull) samples to draw
  K = 11 # number of groups
  D = 10 # number of datapoints (i.e. number of years)
  
  counter_1e7 = 0
  counter_1e6 = 0
  counter_1e5 = 0
  counter_1e4 = 0
  
  y_pred_prior = array(rep(1, N*K*D), dim=c(N, K, D-1)) # initiaalize an empty array to hold data
  
  for (j in 1:K) { # number of age groups
    for (q in 1:N) {
      while (TRUE) {
        alpha = rnorm(1, alpha_prior_mean, alpha_prior_sd) # based on previous studies
        beta = rnorm(1, beta_prior_mean, beta_prior_sd) # based on previous studiess
        
        passed = TRUE
        for (i in 2:D) { # starting with 2nd day becase AR(1) model is used
          mu = alpha + beta * full_grouped_data[j, i-1]
          if (mu < 0) {
            passed = FALSE
            break
          }
        }
        
        if (passed) {
          break
        }
      }
      
      for (i in 2:D) { # starting with 2nd day becase AR(1) model is used
        mu = alpha + beta * full_grouped_data[j, i-1]
        
        if (mu > 0) {
          y_pred = rpois(1, mu)
          
          if (y_pred > 1e7) {
            counter_1e7 = counter_1e7 + 1
          }
          if (y_pred > 1e6) {
            counter_1e6 = counter_1e6 + 1
          }
          if (y_pred > 1e5) {
            counter_1e5 = counter_1e5 + 1
          }
          if (y_pred > 1e4) {
            counter_1e4 = counter_1e4 + 1
          }
  
          y_pred_prior[q, j, i-1] <- y_pred
        }
      }
    }
  }
  
  print(counter_1e7 / (N * K * (D - 1)))
  print(counter_1e6 / (N * K * (D - 1)))
  print(counter_1e5 / (N * K * (D - 1)))
  print(counter_1e4 / (N * K * (D - 1))) 
  
  partial_percentage_hist(y_pred_prior, 1e6)
  partial_percentage_hist(y_pred_prior, 1e5)
  partial_percentage_hist(y_pred_prior, 1e4)
}

get_lfo_cv_results_separate <- function(alpha_prior_sd, beta_prior_sd) {
  N = 10
  L = 4
  K = 11
  
  loglik_exact_separate <- matrix(nrow = 10000, ncol = N * K)
  
  for (i in seq(N, L+1, -1)) {
    full_grouped_data_i <- full_grouped_data[, (1:i)]
    stan_data_i = list(
      N = (i-1) * 11,
      K = 11,
      x = rep(1:nrow(full_grouped_data_i), i - 1),
      y = tail(c(full_grouped_data_i), -11),
      y0 = head(c(full_grouped_data_i), 11),
      alpha_prior_sd = alpha_prior_sd,
      beta_prior_sd = beta_prior_sd,
      use_lfo_cv = 1
    )
    fit_i = sampling(
      stan_model_ar_separate,
      data = stan_data_i,
      seed = 12452,
      refresh=0,
      iter = 5000,
      control=list(max_treedepth=15)
    )
    last_K_values <- seq(((i-2) * K + 1), ((i - 1) * K))
    loglik_exact_separate[, last_K_values] <- extract_log_lik(fit_i)[, last_K_values]
  }
  
  loglik_exact_separate
}

fit_hierarchical_model <- function(
  mu_group_alpha_prior_sd,
  mu_group_beta_prior_sd,
  tau_alpha_prior_sd,
  tau_beta_prior_sd
) {
  nrows <- nrow(full_grouped_data)
  stan_data = list(
    N = (2018-2010+1) * 11,
    K = 11,
    x = rep(1:nrow(full_grouped_data), 9),
    y = tail(c(full_grouped_data), -11),
    y0 = head(c(full_grouped_data), 11),
    mu_group_alpha_prior_sd = mu_group_alpha_prior_sd,
    mu_group_beta_prior_sd = mu_group_beta_prior_sd,
    tau_alpha_prior_sd = tau_alpha_prior_sd,
    tau_beta_prior_sd = tau_beta_prior_sd,
    use_lfo_cv = 0 
  )
  fit_hierarchical = sampling(
    stan_model_ar_hierarchical,
    data = stan_data,
    seed = 12452,
    refresh = 0,
    chains = 7,
    iter = 6000,
    control=list(max_treedepth=15)
  )
  
  fit_hierarchical
}

fit_separate_model <- function(alpha_prior_sd, beta_prior_sd) {
  nrows <- nrow(full_grouped_data)
  stan_data = list(
    N = (2018-2010+1) * 11,
    K = 11,
    x = rep(1:nrow(full_grouped_data), 9),
    y = tail(c(full_grouped_data), -11),
    y0 = head(c(full_grouped_data), 11),
    alpha_prior_sd = alpha_prior_sd,
    beta_prior_sd = beta_prior_sd,
    use_lfo_cv = 0 
  )
  fit_separate = sampling(
    stan_model_ar_separate,
    data = stan_data,
    seed = 12452,
    refresh=0,
    chains = 7,
    iter = 6000
  )
  
  fit_separate
}

plot_ppc_ribbons <- function(fit, true_data) {
  y_rep <- as.matrix(fit, pars = "y_rep")
  plots <- lapply(
    seq(1, 11),
    function(group_idx) {
      indexes <- seq(group_idx, 99, 11)
      ppc_ribbon(c(true_data[group_idx, (2:10)]), y_rep[, indexes])
    }
  )
  
  grid.arrange(grobs = plots[1:6], ncol = 2)
  grid.arrange(grobs = plots[7:11], ncol = 2) 
}

plot_ppc_intervals <- function(fit, true_data) {
  y_rep <- as.matrix(fit, pars = "y_rep")
  plots <- lapply(
    seq(1, 11),
    function(group_idx) {
      indexes <- seq(group_idx, 99, 11)
      ppc_intervals(c(true_data[group_idx, (2:10)]), y_rep[, indexes])
    }
  )
  
  plots 
}
```


# Introduction

// this part has to be inviting xD
// General talk about Finalnd having high rates of domestic violence, need to raise awareness, etc.

// Main topic/purpose of the project/notebook.

// Present outline of the notebook 

// structure and organization has to be logical

# Data description

Here is a tabular representation of the data.

```{r}
data = read.csv("dataset.csv", header = TRUE, sep=";")
data
```

Below are some utility functions copied from https://github.com/avehtari/BDA_R_demos/blob/master/demos_rstan/stan_utility.R.
The functions are used later mainly for doing model diagnostics of different kinds 

```{r}
source('stan_utility.R')
lsf.str()
```

Below are scatter plots for each of the age groups

```{r}
ageGroups <- unique(data$Victim.s.age)
grouped_data <- c()

plots <- lapply(
  seq(1, 11),
  function(i) {
    data_for_age_group <- filter(data, Victim.s.age == ageGroups[i])
     ggplot(aes(Year, Number.of.victims), data = data_for_age_group) +
      geom_point(size = 1)  + geom_line()  
  }
)

grid.arrange(grobs = plots[1:6], ncol = 2)
grid.arrange(grobs = plots[7:11], ncol = 2)
```

Preparing the data

```{r}
full_grouped_data <- sapply(
  2009:2018,
  function (j) sapply(
    1:11,
    function (i) filter(
      data,
      Victim.s.age == ageGroups[i] & Year == j
    )$Number.of.victims
  )
)
full_grouped_data
```

# Analysis problem

# Models
// add a clear list of models

## Model 1

The model certainly has some issues, so let's try instead  the complementary non-centered parameterization (the idea to use it to improve the issues mentioned above was adapted from here https://mc-stan.org/users/documentation/case-studies/rstan_workflow.html and here https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html)


```{stan, output.var="stan_model_ar_hierarchical"}
data {
  int<lower=0> N;
  int<lower=0> K; // number of groups
  int<lower=1,upper=K> x[N]; // group indicator
  int<lower=0> y[N];
  int<lower=0> y0[K];
  real<lower=0> mu_group_alpha_prior_sd;
  real<lower=0> mu_group_beta_prior_sd;
  real<lower=0> tau_alpha_prior_sd;
  real<lower=0> tau_beta_prior_sd;
  int<lower=0, upper=1> use_lfo_cv;
}
parameters {
  real mu_group_alpha;
  real<lower=0> mu_group_beta;
  real<lower=0> tau_alpha; // prior std
  real<lower=0> tau_beta; // prior std
  vector[K] alpha_tilde;
  real<lower=0> beta_tilde[K];
}
transformed parameters {
  real<lower=0> mu[use_lfo_cv ? (N - K) : N];
  
  real alpha[K];
  real beta[K];
  
  for (i in 1:K) {
    alpha[i] = mu_group_alpha + tau_alpha * alpha_tilde[i];
    beta[i] = mu_group_beta + tau_beta * beta_tilde[i];
  }
  
  for (n in 1:K) {
    mu[n] = alpha[x[n]] + beta[x[n]] * y0[n];
  }
  
  for (n in K+1:(use_lfo_cv ? (N - K) : N)) {
    mu[n] = alpha[x[n]] + beta[x[n]] * y[n-K];
  }
}
model {
  mu_group_alpha ~ normal(0, mu_group_alpha_prior_sd); // prior
  mu_group_beta ~ normal(0, mu_group_beta_prior_sd); // weakly informative prior
  tau_alpha ~ cauchy(0, tau_alpha_prior_sd); // weakly informative prior
  tau_beta ~ cauchy(0, tau_beta_prior_sd); // weakly informative prior
  
  alpha_tilde ~ normal(0, 1);
  beta_tilde ~ normal(0, 1);
  
  for (n in 1:(use_lfo_cv ? (N - K) : N)) {
     y[n] ~ poisson(mu[n]);
  }
}
generated quantities {
  vector[N] log_lik;
  real<lower=0> mu_pred[use_lfo_cv ? K : 0];
  vector[use_lfo_cv ? 0 : N] y_rep;
  
  if (use_lfo_cv) {
    for (n in 1:K) {
      mu_pred[n] = alpha[n] + beta[n] * y[(N - K) - K + n];
    }
    
    for (i in 1:(N - K)) {
      log_lik[i] = poisson_lpmf(y[i] | mu[i]);
    }
    
    for (i in 1:K) {
      log_lik[(N - K) + i] = poisson_lpmf(y[(N - K) + i] | mu_pred[i]);
    }
  } else {
    for (i in 1:N) {
      y_rep[i] = poisson_rng(mu[i]);
      log_lik[i] = poisson_lpmf(y[i] | mu[i]);
    }
  }
}
```

### Prior predictive checking

```{r}
check_prior_hierarchical(full_grouped_data, 20000, 20, 5000, 20)
```

```{r}
check_prior_hierarchical(full_grouped_data, 20000, 5, 1000, 2)
```

```{r}
check_prior_hierarchical(full_grouped_data, 10000, 2, 200, 2)
```

The latter prior seems to be the most reasonable one. About 52.5% of the mass lies between 0 and 10000. About 6% of the mass is with values > 100000.

This is vage enough and reasonably constrained at the same time.


### Model diagnostics


```{r}
fit_hierarchical <- fit_hierarchical_model(10000, 2, 200, 2)
print(fit_hierarchical)
```

```{r}
print_fit_diagnostic(fit_hierarchical, 15)
```


*PSIS-LOO*

```{r}
loo_hierarchical <- get_loo_diagnostics(fit_hierarchical)
print(loo_hierarchical)
plot(loo_hierarchical)
```

The code below is adapted from http://mc-stan.org/loo/articles/loo2-lfo.html


TODO: add reference to Aki's code

```{r}
loglik_exact_hierarchical <- get_lfo_cv_results_hierarchical(10000, 2, 200, 2)
exact_elpds_1sap <- apply(loglik_exact_hierarchical, 2, log_mean_exp)
exact_elpd_hierarchical <- c(ELPD = sum(exact_elpds_1sap[34:99]))

rbind_print(
  "LFO" = exact_elpd_hierarchical
)
```

### Graphical posterior predictive checking

```{r}
plot_ppc_ribbons(fit_hierarchical, full_grouped_data)
```

```{r}
plot_ppc_intervals(fit_hierarchical, full_grouped_data)
```

```{r}
y_rep <- as.matrix(fit_hierarchical, pars = "y_rep")
ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep)
```


### Prior sensitivity analysis

Now let's try another sensible but a little stronger informative prior, while keeping results of the previous fit as our base-line.

```{r}
fit_hierarchical_wide_prior = fit_hierarchical_model(10000, 25, 2000, 25) # wider prior
print(fit_hierarchical_wide_prior)
```

```{r}
print_fit_diagnostic(fit_hierarchical_wide_prior, 15)
```

```{r}
loo_hierarchical_wide_prior <- get_loo_diagnostics(fit_hierarchical_wide_prior)
print(loo_hierarchical_wide_prior)
plot(loo_hierarchical_wide_prior)
```

```{r}
plot_ppc_ribbons(fit_hierarchical_wide_prior, full_grouped_data)
```

```{r}
plot_ppc_intervals(fit_hierarchical_wide_prior, full_grouped_data)
```

```{r}
loglik_exact_hierarchical <- get_lfo_cv_results_hierarchical(10000, 25, 2000, 25)
exact_elpds_1sap <- apply(loglik_exact_hierarchical, 2, log_mean_exp)
exact_elpd_hierarchical_wide_prior <- c(ELPD = sum(exact_elpds_1sap[34:99]))

rbind_print(
  "LFO" = exact_elpd_hierarchical_wide_prior
)
```


Now let's try a stronger more narrow prior

```{r}
fit_hierarchical_narrow_prior = fit_hierarchical_model(2000, 0.5, 10, 0.1)
print(fit_hierarchical_narrow_prior)
```

```{r}
print_fit_diagnostic(fit_hierarchical_narrow_prior, 15)
```

```{r}
loo_hierarchical_narrow_prior <- get_loo_diagnostics(fit_hierarchical_narrow_prior)
print(loo_hierarchical_narrow_prior)
plot(loo_hierarchical_narrow_prior)
```

```{r}
plot_ppc_ribbons(fit_hierarchical_narrow_prior, full_grouped_data)
```

```{r}
plot_ppc_intervals(fit_hierarchical_narrow_prior, full_grouped_data)
```

```{r}
loglik_exact_hierarchical <- get_lfo_cv_results_hierarchical(2000, 0.5, 10, 0.1)
exact_elpds_1sap <- apply(loglik_exact_hierarchical, 2, log_mean_exp)
exact_elpd_hierarchical_narrow_prior <- c(ELPD = sum(exact_elpds_1sap[34:99]))

rbind_print(
  "LFO" = exact_elpd_hierarchical_narrow_prior
)
```

```{r}
y_rep <- as.matrix(fit_hierarchical, pars = "y_rep")
y_rep_wide_prior <- as.matrix(fit_hierarchical_wide_prior, pars = "y_rep")
y_rep_narrow_prior <- as.matrix(fit_hierarchical_narrow_prior, pars = "y_rep")
plot_baseline = ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep)
plot_wide_prior = ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep_wide_prior)
plot_narrow_prior = ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep_narrow_prior)

plot(plot_wide_prior)
plot(plot_baseline)
plot(plot_narrow_prior)
```

// include proper prior, jusify

// Rhat convergence diagnostics

// HMC specific convergence diagnostics (divergences, tree depth)

// ESS diagnostics

// posterior predictive checking

## Model 2

```{stan, output.var="stan_model_ar_separate"}
data {
  int<lower=0> N;
  int<lower=0> K; // number of groups
  int<lower=1,upper=K> x[N]; // group indicator
  int<lower=0> y[N];
  int<lower=0> y0[K];
  real<lower=0> alpha_prior_sd;
  real<lower=0> beta_prior_sd;
  int<lower=0, upper=1> use_lfo_cv;
}
parameters {
  real alpha[K];
  real<lower=0> beta[K];
}
transformed parameters {
  real<lower=0> mu[use_lfo_cv ? (N - K) : N];
  
  for (n in 1:K) {
    mu[n] = alpha[x[n]] + beta[x[n]] * y0[n];
  }
  
  for (n in K+1:(use_lfo_cv ? (N - K) : N)) {
    mu[n] = alpha[x[n]] + beta[x[n]] * y[n-K];
  }
}
model {
  for (n in 1:K) {
    alpha[n] ~ normal(0, alpha_prior_sd); // weakly informative prior
    beta[n] ~ normal(0, beta_prior_sd);  // weakly informative prior
  }
  
  for (n in 1:(use_lfo_cv ? (N - K) : N)) {
     y[n] ~ poisson(mu[n]);
  }
}
generated quantities {
  vector[N] log_lik;
  real<lower=0> mu_pred[use_lfo_cv ? K : 0];
  vector[use_lfo_cv ? 0 : N] y_rep;
  
  if (use_lfo_cv) {
    for (n in 1:K) {
      mu_pred[n] = alpha[n] + beta[n] * y[(N - (1 * K)) - K + n];
    }
    
    for (i in 1:(N - (1 * K))) {
      log_lik[i] = poisson_lpmf(y[i] | mu[i]);
    }
    
    for (i in 1:K) {
      log_lik[(N - (1 * K)) + i] = poisson_lpmf(y[(N - (1 * K)) + i] | mu_pred[i]);
    }
  } else {
    for (i in 1:N) {
      y_rep[i] = poisson_rng(mu[i]);
      log_lik[i] = poisson_lpmf(y[i] | mu[i]);
    }
  }
}
```


### Prior predictive checking

```{r}
check_prior_separate(full_grouped_data, 0, 10000, 0, 2)
```

```{r}
check_prior_separate(full_grouped_data, 0, 10000, 1, 1)
```

```{r}
check_prior_separate(full_grouped_data, 0, 10000, 0, 25)
```

```{r}
check_prior_separate(full_grouped_data, 0, 30000, 0, 2)
```


```{r}
fit_separate = fit_separate_model(10000, 2)
print(fit_separate)
```

```{r}
print_fit_diagnostic(fit_separate, 10)
```

*PSIS-LOO*

```{r}
loo_separate <- get_loo_diagnostics(fit_separate)
print(loo_separate)
plot(loo_separate)
```

The code below is adapted from http://mc-stan.org/loo/articles/loo2-lfo.html

```{r}
loglik_exact_separate <- get_lfo_cv_results_separate(10000, 2)
exact_elpds_1sap <- apply(loglik_exact_separate, 2, log_mean_exp)
exact_elpd_separate <- c(ELPD = sum(exact_elpds_1sap[34:99]))

rbind_print(
  "LFO" = exact_elpd_separate
)
```

### Graphical posterior predictive checking

```{r}
plot_ppc_ribbons(fit_separate, full_grouped_data)
```


```{r}
plot_ppc_intervals(fit_separate, full_grouped_data)
```

```{r}
y_rep <- as.matrix(fit_separate, pars = "y_rep")
ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep)
```

### Prior sensitivity analysis

Now let's try another sensible but a little stronger informative prior, while keeping results of the previous fit as our base-line.
```{r}
fit_separate_wide_prior = fit_separate_model(10000, 25)
print(fit_separate_wide_prior)
```

```{r}
print_fit_diagnostic(fit_separate_wide_prior, 10)
```

```{r}
loo_separate_wide_prior <- get_loo_diagnostics(fit_separate_wide_prior)
print(loo_separate_wide_prior)
plot(loo_separate_wide_prior)
```


```{r}
plot_ppc_ribbons(fit_separate_wide_prior, full_grouped_data)
```

```{r}
loglik_exact_separate <- get_lfo_cv_results_separate(10000, 25)
exact_elpds_1sap <- apply(loglik_exact_separate, 2, log_mean_exp)
exact_elpd_separate_wide_prior <- c(ELPD = sum(exact_elpds_1sap[34:99]))

rbind_print(
  "LFO" = exact_elpd_separate_wide_prior
)
```


Now let's try a stronger more narrow prior

```{r}
fit_separate_narrow_prior = fit_separate_model(2000, 0.5)
print(fit_separate_narrow_prior)
```

```{r}
print_fit_diagnostic(fit_separate_narrow_prior, 10)
```

```{r}
loo_separate_narrow_prior <- get_loo_diagnostics(fit_separate_narrow_prior)
print(loo_separate_narrow_prior)
plot(loo_separate_narrow_prior)
```

```{r}
loglik_exact_separate <- get_lfo_cv_results_separate(2000, 0.5)
exact_elpds_1sap <- apply(loglik_exact_separate, 2, log_mean_exp)
exact_elpd_separate_narrow_prior <- c(ELPD = sum(exact_elpds_1sap[34:99]))

rbind_print(
  "LFO" = exact_elpd_separate_narrow_prior
)
```

```{r}
y_rep <- as.matrix(fit_separate, pars = "y_rep")
y_rep_wide_prior <- as.matrix(fit_separate_wide_prior, pars = "y_rep")
y_rep_narrow_prior <- as.matrix(fit_separate_narrow_prior, pars = "y_rep")
plot_baseline = ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep)
plot_wide_prior = ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep_wide_prior)
plot_narrow_prior = ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep_narrow_prior)

plot(plot_wide_prior)
plot(plot_baseline)
plot(plot_narrow_prior)
```

## Model comparison

Here is a comparison for hierarchical and separate model respectively

```{r}
ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), as.matrix(fit_hierarchical, pars = "y_rep"))
ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), as.matrix(fit_separate, pars = "y_rep"))
```

```{r}
comparison_results = matrix(
  c(
    loo_hierarchical$estimates[1:2], # elpd and p_eff values
    exact_elpd_hierarchical,
    loo_hierarchical_wide_prior$estimates[1:2], # elpd and p_eff values
    exact_elpd_hierarchical_wide_prior,
    loo_hierarchical_narrow_prior$estimates[1:2], # elpd and p_eff values
    exact_elpd_hierarchical_narrow_prior,
    loo_separate$estimates[1:2], # elpd and p_eff values
    exact_elpd_separate,
    loo_separate_wide_prior$estimates[1:2], # elpd and p_eff values
    exact_elpd_separate_wide_prior,
    loo_separate_narrow_prior$estimates[1:2], # elpd and p_eff values
    exact_elpd_separate_narrow_prior
  ),
  ncol = 6,
  dimnames = list(
    c("elpd_loo", "p_loo", "elpd_lfo"),
    c("hierarchical",
      "hierarchical (wide prior)",
      "hierarchical (narrow prior)",
      "separate",
      "separate (wide prior)",
      "separate (narrow prior)")
  )
)
comparison_results
```

```{r}
kable(comparison_results)
```

// include proper prior, jusify

// Rhat convergence diagnostics

// HMC specific convergence diagnostics (divergences, tree depth)

// ESS diagnostics

// posterior predictive checking

# Problems encountered and potential improvements


# Conclusion

// a clear conclusion here
