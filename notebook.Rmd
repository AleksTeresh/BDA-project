---
title: "BDA project"
author: "Anonymous"
output:
  pdf_document:
    toc: yes
    toc_depth: 1
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '1'
  word_document:
    toc: yes
    toc_depth: '1'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Loaded packages

```{r, echo=TRUE}
suppressPackageStartupMessages({
  library(tidyr)
  library(dplyr)
  library(knitr)
  library(rstan)
  library(rstanarm)
  library(bayesplot)
  library(aaltobda)
  options(mc.cores = parallel::detectCores())
  library(loo)
  library(ggplot2)
  library(gridExtra)
  library(bayesplot)
})
```

```{r}
SEED <- 1472652
set.seed(SEED)
options(max.print=1000000)
```

# Utility functions

```{r}
# The code in stan_utility.R was copied from https://github.com/avehtari/BDA_R_demos/blob/master/demos_rstan/stan_utility.R
source('stan_utility.R')

# plot a histogram with percentages instead of frequencies 
partial_percentage_hist <- function(
  data,
  upper_limit
) {
  truncated_data = data[data < upper_limit]
  h <- hist(truncated_data, plot = FALSE)
  h$density <- h$counts / sum(h$counts) * 100
  plot(h, freq=F, ylab='Percentage', col = '#6eb5db')
}

# prior preditive checking for hierarchical model
check_prior_hierarchical <- function(
  full_grouped_data, 
  mu_group_alpha_sd,
  mu_group_beta_sd,
  tau_alpha_sd,
  tau_beta_sd
) {
  N = 5000 # number of samples to draw
  K = 11 # number of hierarchical groups
  D = 10 # number of datapoints for each group (i.e. number of years)
  
  counter_1e7 = 0
  counter_1e6 = 0
  counter_1e5 = 0
  counter_1e4 = 0
  
  y_pred_prior = array(rep(1, N*K*D), dim=c(N, K, D-1)) # initialize an empty array to hold data
  
  for (j in 1:K) { # number of age groups
    mu_group_alpha = rnorm(N, 0, mu_group_alpha_sd)
      
    for (q in 1:N) {
      mu_group_beta = rnorm(1, 0, mu_group_beta_sd)
      
      while (mu_group_beta < 0) {
        mu_group_beta = rnorm(1, 0, mu_group_beta_sd)
      }
      
      while (TRUE) {
        tau_alpha = rcauchy(1, 0, tau_alpha_sd)
        tau_beta = rcauchy(1, 0, tau_beta_sd)
        
        while (tau_alpha < 0 || tau_beta < 0) {
          tau_alpha = rcauchy(1, 0, tau_alpha_sd)
          tau_beta = rcauchy(1, 0, tau_beta_sd)
        }
        
        alpha = rnorm(1, mu_group_alpha[q], tau_alpha)
        beta = rnorm(1, mu_group_beta, tau_beta)
        
        if (beta >= 0) {
          passed = TRUE
          # starting with 2nd day becase AR(1) model is used (no meaningful fit for the first point)
          for (i in 2:D) { 
            mu = alpha + beta * full_grouped_data[j, i-1]
            if (mu < 0) {
              passed = FALSE
              break
            }
          }
          
          if (passed) {
            break
          }  
        }
      }
      
      # starting with 2nd day becase AR(1) model is used
      for (i in 2:D) { 
        mu = alpha + beta * full_grouped_data[j, i-1]
        
        if (mu > 0) {
          y_pred = rpois(1, mu)
          
          if (y_pred > 1e7) {
            counter_1e7 = counter_1e7 + 1
          }
          if (y_pred > 1e6) {
            counter_1e6 = counter_1e6 + 1
          }
          if (y_pred > 1e5) {
            counter_1e5 = counter_1e5 + 1
          }
          if (y_pred > 1e4) {
            counter_1e4 = counter_1e4 + 1
          }
    
          y_pred_prior[q, j, i-1] <- y_pred
        }
      }
    }
  }
  
  print(paste0('Percentage of the mass that are greater than 10,000,000:  ' , round(counter_1e7 * 100 / (N * K * (D - 1)), 3)))
  print(paste0('Percentage of the mass that are greater than 1,000,000:  ' , round(counter_1e6 * 100 / (N * K * (D - 1)), 3))) 
  print(paste0('Percentage of the mass that are greater than 100,000:  ' , round(counter_1e5 * 100/ (N * K * (D - 1)),3)))
  print(paste0('Percentage of the mass that are greater than 10,000:  ' , round(counter_1e4 * 100/ (N * K * (D - 1)),3)))
  
  plot1 <- partial_percentage_hist(y_pred_prior, 1e6)
  plot2 <- partial_percentage_hist(y_pred_prior, 1e5)
  plot3 <- partial_percentage_hist(y_pred_prior, 1e4)
}

# HMC specific convergence diagnostics
print_fit_diagnostic <- function(fit, tree_depth) {
  check_treedepth(fit, tree_depth)
  check_div(fit)
}

# do PSIS-LOO for the fit ommitting first 4 years out of 10
get_loo_diagnostics <- function(fit) {
  N <- 99
  # our fit does not have data for the first year already, so we only need to remove 3 years from the beginning
  L <- 11 * 3
  
  log_lik_all <- extract_log_lik(fit, merge_chains = FALSE)
  log_lik <- log_lik_all[, , (L+1):N]
  
  r_eff <- relative_eff(exp(log_lik))
  loo(log_lik, r_eff = r_eff)
}

########
# The code below is adapted from http://mc-stan.org/loo/articles/loo2-lfo.html
# more stable than log(sum(exp(x))) 
log_sum_exp <- function(x) {
  max_x <- max(x)  
  max_x + log(sum(exp(x - max_x)))
}

# more stable than log(mean(exp(x)))
log_mean_exp <- function(x) {
  log_sum_exp(x) - log(length(x))
}

# compute log of raw importance ratios
# sums over observations *not* over posterior samples
sum_log_ratios <- function(ll, ids = NULL) {
  if (!is.null(ids)) ll <- ll[, ids, drop = FALSE]
  - rowSums(ll)
}

# for printing comparisons later
rbind_print <- function(...) {
  round(rbind(...), digits = 2)
}

get_lfo_cv_results_hierarchical <- function(
  mu_group_alpha_prior_sd,
  mu_group_beta_prior_sd,
  tau_alpha_prior_sd,
  tau_beta_prior_sd
) {
  N = 10 # numebr of data points for eahc group
  L = 4 # number of datapoints to skip from the start
  K = 11 # number of age groups
  
  loglik_exact_hierarchical <- matrix(nrow = 10000, ncol = N * K)
  
  for (i in seq(N, L+1, -1)) {
    full_grouped_data_i <- full_grouped_data[, (1:i)]
    stan_data_i = list(
      N = (i-1) * 11,
      K = 11,
      x = rep(1:nrow(full_grouped_data_i), i - 1),
      y = tail(c(full_grouped_data_i), -11),
      y0 = head(c(full_grouped_data_i), 11),
      mu_group_alpha_prior_sd = mu_group_alpha_prior_sd,
      mu_group_beta_prior_sd = mu_group_beta_prior_sd,
      tau_alpha_prior_sd = tau_alpha_prior_sd,
      tau_beta_prior_sd = tau_beta_prior_sd,
      use_lfo_cv = 1
    )
    fit_i = sampling(
      stan_model_ar_hierarchical,
      data = stan_data_i,
      seed = 12452,
      refresh=0,
      iter = 5000,
      control=list(max_treedepth=15)
    )
    last_K_values <- seq(((i-2) * K + 1), ((i - 1) * K))
    loglik_exact_hierarchical[, last_K_values] <- extract_log_lik(fit_i)[, last_K_values]
  }
  
  exact_elpds_1sap <- apply(loglik_exact_hierarchical, 2, log_mean_exp)
  
  c(ELPD = sum(exact_elpds_1sap[34:99]))
}
#####
# End of the code adapted from http://mc-stan.org/loo/articles/loo2-lfo.html

check_prior_separate <- function(
  full_grouped_data, 
  alpha_prior_mean,
  alpha_prior_sd,
  beta_prior_mean,
  beta_prior_sd
) {
  N = 3000 # number of (succesfull) samples to draw
  K = 11 # number of groups
  D = 10 # number of datapoints (i.e. number of years)
  
  counter_1e7 = 0
  counter_1e6 = 0
  counter_1e5 = 0
  counter_1e4 = 0
  
  # initiaalize an empty array to hold data
  y_pred_prior = array(rep(1, N*K*D), dim=c(N, K, D-1))
  
  for (j in 1:K) { # number of age groups
    for (q in 1:N) {
      while (TRUE) {
        alpha = rnorm(1, alpha_prior_mean, alpha_prior_sd)
        beta = rnorm(1, beta_prior_mean, beta_prior_sd)
        
        passed = TRUE
         # starting with 2nd day becase AR(1) model is used
        for (i in 2:D) {
          mu = alpha + beta * full_grouped_data[j, i-1]
          if (mu < 0) {
            passed = FALSE
            break
          }
        }
        
        if (passed) {
          break
        }
      }
      
      for (i in 2:D) { # starting with 2nd day becase AR(1) model is used
        mu = alpha + beta * full_grouped_data[j, i-1]
        
        if (mu > 0) {
          y_pred = rpois(1, mu)
          
          if (y_pred > 1e7) {
            counter_1e7 = counter_1e7 + 1
          }
          if (y_pred > 1e6) {
            counter_1e6 = counter_1e6 + 1
          }
          if (y_pred > 1e5) {
            counter_1e5 = counter_1e5 + 1
          }
          if (y_pred > 1e4) {
            counter_1e4 = counter_1e4 + 1
          }
  
          y_pred_prior[q, j, i-1] <- y_pred
        }
      }
    }
  }
  
  print(paste0('Percentage of the mass that are greater than 10,000,000:  ' , round(counter_1e7 * 100 / (N * K * (D - 1)), 3)))
  print(paste0('Percentage of the mass that are greater than 1,000,000:  ' , round(counter_1e6 * 100 / (N * K * (D - 1)), 3))) 
  print(paste0('Percentage of the mass that are greater than 100,000:  ' , round(counter_1e5 * 100/ (N * K * (D - 1)),3)))
  print(paste0('Percentage of the mass that are greater than 10,000:  ' , round(counter_1e4 * 100/ (N * K * (D - 1)),3)))
  
  partial_percentage_hist(y_pred_prior, 1e6)
  partial_percentage_hist(y_pred_prior, 1e5)
  partial_percentage_hist(y_pred_prior, 1e4)
}

# LFO-CV for separate model
get_lfo_cv_results_separate <- function(alpha_prior_sd, beta_prior_sd) {
  N = 10
  L = 4
  K = 11
  
  loglik_exact_separate <- matrix(nrow = 10000, ncol = N * K)
  
  for (i in seq(N, L+1, -1)) {
    full_grouped_data_i <- full_grouped_data[, (1:i)]
    stan_data_i = list(
      N = (i-1) * 11,
      K = 11,
      x = rep(1:nrow(full_grouped_data_i), i - 1),
      y = tail(c(full_grouped_data_i), -11),
      y0 = head(c(full_grouped_data_i), 11),
      alpha_prior_sd = alpha_prior_sd,
      beta_prior_sd = beta_prior_sd,
      use_lfo_cv = 1
    )
    fit_i = sampling(
      stan_model_ar_separate,
      data = stan_data_i,
      seed = 12452,
      refresh=0,
      iter = 5000,
      control=list(max_treedepth=15)
    )
    last_K_values <- seq(((i-2) * K + 1), ((i - 1) * K))
    loglik_exact_separate[, last_K_values] <- extract_log_lik(fit_i)[, last_K_values]
  }
  
  exact_elpds_1sap <- apply(loglik_exact_separate, 2, log_mean_exp)
  c(ELPD = sum(exact_elpds_1sap[34:99]))
}

fit_hierarchical_model <- function(
  mu_group_alpha_prior_sd,
  mu_group_beta_prior_sd,
  tau_alpha_prior_sd,
  tau_beta_prior_sd
) {
  nrows <- nrow(full_grouped_data)
  stan_data = list(
    N = (2018-2010+1) * 11,
    K = 11,
    x = rep(1:nrow(full_grouped_data), 9),
    y = tail(c(full_grouped_data), -11),
    y0 = head(c(full_grouped_data), 11),
    mu_group_alpha_prior_sd = mu_group_alpha_prior_sd,
    mu_group_beta_prior_sd = mu_group_beta_prior_sd,
    tau_alpha_prior_sd = tau_alpha_prior_sd,
    tau_beta_prior_sd = tau_beta_prior_sd,
    use_lfo_cv = 0 
  )
  fit_hierarchical = sampling(
    stan_model_ar_hierarchical,
    data = stan_data,
    seed = 12452,
    refresh = 0,
    chains = 7,
    iter = 6000,
    control=list(max_treedepth=15)
  )
  
  fit_hierarchical
}

fit_separate_model <- function(alpha_prior_sd, beta_prior_sd) {
  nrows <- nrow(full_grouped_data)
  stan_data = list(
    N = (2018-2010+1) * 11,
    K = 11,
    x = rep(1:nrow(full_grouped_data), 9),
    y = tail(c(full_grouped_data), -11),
    y0 = head(c(full_grouped_data), 11),
    alpha_prior_sd = alpha_prior_sd,
    beta_prior_sd = beta_prior_sd,
    use_lfo_cv = 0 
  )
  fit_separate = sampling(
    stan_model_ar_separate,
    data = stan_data,
    seed = 12452,
    refresh=0,
    chains = 7,
    iter = 6000
  )
  
  fit_separate
}

plot_ppc_ribbons <- function(fit, true_data) {
  y_rep <- as.matrix(fit, pars = "y_rep")
  plots <- lapply(
    seq(1, 11),
    function(group_idx) {
      indexes <- seq(group_idx, 99, 11)
      ppc_ribbon(c(true_data[group_idx, (2:10)]), y_rep[, indexes])
    }
  )
  
  grid.arrange(grobs = plots[1:6], ncol = 2)
  grid.arrange(grobs = plots[7:11], ncol = 2) 
}

plot_ppc_intervals <- function(fit, true_data) {
  y_rep <- as.matrix(fit, pars = "y_rep")
  plots <- lapply(
    seq(1, 11),
    function(group_idx) {
      indexes <- seq(group_idx, 99, 11)
      ppc_intervals(c(true_data[group_idx, (2:10)]), y_rep[, indexes])
    }
  )
  
  plots 
}
```


# Introduction

Finland is often praised for their achievements on safe enviorment to live in yet a [report](https://yle.fi/uutiset/osasto/news/article10531775.ece) by Finnish National Institute for Health and Welfare indicated that there're 130,000 people were victims of violence. In this project, we study the number of domestic violence cases happen among several age groups and build statistical model for predicting trend in domestic violence.

# Data description

The dataset come from Tilastokeskus (Statistic Finland) [https://yle.fi/uutiset/osasto/news/increase_in_reported_domestic_violence_cases_in_finland/10818387] and contains data about number of victims by age from 2009 to 2018. There are totally 11 age groups, which are [0-4], [5-9], [10-14], [15-17], [18-20], [21-24], [25-34], [35-44], [45-54], [55-64] and [65-].

```{r}
data = read.csv("dataset.csv", header = TRUE, sep=";")
data
```

We visuallize the number of victim per year for each age group 

```{r}
ageGroups <- unique(data$Victim.s.age)
grouped_data <- c()

plots <- lapply(
  seq(1, 11),
  function(i) {
    data_for_age_group <- filter(data, Victim.s.age == ageGroups[i])
     ggplot(aes(Year, Number.of.victims), data = data_for_age_group) +
      geom_point(size = 1)  + geom_line()  
  }
)

grid.arrange(grobs = plots[1:6], ncol = 2)
grid.arrange(grobs = plots[7:11], ncol = 2)
```

We reformat the data into matrix form so that it's easier to work with. Each row of the matrix represents number of victims by age, from [-4] to [65-], in a year, starting from 2009 to 2018.

```{r}
full_grouped_data <- sapply(
  2009:2018,
  function (j) sapply(
    1:11,
    function (i) filter(
      data,
      Victim.s.age == ageGroups[i] & Year == j
    )$Number.of.victims
  )
)
full_grouped_data
```

# Analysis problem

The purpose of the data analysis is to build a model that's capable of predicting the number of domestic violence victims for each age group in the future. We decided to select the separate and hierachial model and compare their performance based on ELPD value.

# Models

The dataset features data points in chronological order so it's in our best interest to use time series approach. The model we used is called Autoregressive Model, which is the most commonly used time-series models. Since the dataset only consists data of 10 years, we decided to use the first-order autoregressive model AR(1) which means that the value of year N would depends on year N-1.

There are 2 models in this notebook:

1. Heirarchical AR(1) model

2. Separate AR(1) model

To model the number of victim per year, we used Poisson distribution which is popular for modeling the number of times an event occurs in an interval of time. This way, the number of victim in year $y_n$ of each age group will be propotional to the following distribution:

$$
y_n \sim poisson(\alpha + \beta y_{n-1})
$$

## Model 1

This is the hierachial AR(1) model. Since we don't have much data and the inter group variation is high we decided to used the non-centered parameterization approach for the hierachial model (the idea to use it to improve the issues mentioned above was adapted from here https://mc-stan.org/users/documentation/case-studies/rstan_workflow.html and here https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html ) rather than the centered one.


```{stan, output.var="stan_model_ar_hierarchical"}
data {
  int<lower=0> N;
  int<lower=0> K; // number of groups
  int<lower=1,upper=K> x[N]; // group indicator
  int<lower=0> y[N];
  int<lower=0> y0[K];
  real<lower=0> mu_group_alpha_prior_sd;
  real<lower=0> mu_group_beta_prior_sd;
  real<lower=0> tau_alpha_prior_sd;
  real<lower=0> tau_beta_prior_sd;
  int<lower=0, upper=1> use_lfo_cv;
}
parameters {
  real mu_group_alpha;
  real<lower=0> mu_group_beta;
  real<lower=0> tau_alpha; // hierarchical std
  real<lower=0> tau_beta; // hierarchical std
  vector[K] alpha_tilde;
  real<lower=0> beta_tilde[K];
}
transformed parameters {
  real<lower=0> mu[use_lfo_cv ? (N - K) : N];
  
  real alpha[K];
  real beta[K];
  
  for (i in 1:K) {
    alpha[i] = mu_group_alpha + tau_alpha * alpha_tilde[i];
    beta[i] = mu_group_beta + tau_beta * beta_tilde[i];
  }
  
  for (n in 1:K) {
    mu[n] = alpha[x[n]] + beta[x[n]] * y0[n];
  }
  
  for (n in K+1:(use_lfo_cv ? (N - K) : N)) {
    mu[n] = alpha[x[n]] + beta[x[n]] * y[n-K];
  }
}
model {
  mu_group_alpha ~ normal(0, mu_group_alpha_prior_sd); // hierarchical prior for mean
  mu_group_beta ~ normal(0, mu_group_beta_prior_sd); // hierarchical prior for mean
  tau_alpha ~ cauchy(0, tau_alpha_prior_sd); // hierarchical prior for std
  tau_beta ~ cauchy(0, tau_beta_prior_sd); // hierarchical prior for std
  
  alpha_tilde ~ normal(0, 1);
  beta_tilde ~ normal(0, 1);
  
  for (n in 1:(use_lfo_cv ? (N - K) : N)) {
     y[n] ~ poisson(mu[n]);
  }
}
generated quantities {
  vector[N] log_lik;
  real<lower=0> mu_pred[use_lfo_cv ? K : 0];
  vector[use_lfo_cv ? 0 : N] y_rep;
  
  if (use_lfo_cv) {
    for (n in 1:K) {
      mu_pred[n] = alpha[n] + beta[n] * y[(N - K) - K + n];
    }
    
    for (i in 1:(N - K)) {
      log_lik[i] = poisson_lpmf(y[i] | mu[i]);
    }
    
    for (i in 1:K) {
      log_lik[(N - K) + i] = poisson_lpmf(y[(N - K) + i] | mu_pred[i]);
    }
  } else {
    for (i in 1:N) {
      y_rep[i] = poisson_rng(mu[i]);
      log_lik[i] = poisson_lpmf(y[i] | mu[i]);
    }
  }
}
```

### Prior predictive checking

We do prior predictive checking bassed on the general knowledge about the Finnish population. We believe that a major part of the mass should lies between 0 and 10000 since the population of Finland is around 5.2 millions. 

We implemented the prior predictive checking funcion. The parameters consists of the dataset, prior for mean and standard deviation of $\alpha$ and prior for mean and standard deviation of $\beta$.

```{r}
check_prior_hierarchical(full_grouped_data, 20000, 20, 5000, 20)
```

Roughtly 86.4% of the mass are greater than 10,000 so this is not a reasonable choice for prior.

```{r}
check_prior_hierarchical(full_grouped_data, 20000, 5, 1000, 2)
```

Around 70% of the mass are greater than 10,000 so the prior choice is still bad. We move on to the third choice.

```{r}
check_prior_hierarchical(full_grouped_data, 10000, 2, 2000, 2)
```

The latter prior seems to be the most reasonable one. About 50% of the mass lies between 0 and 10000. About 5% of the mass is with values > 100000.

This is vage enough and reasonably constrained at the same time.

### Model diagnostics

```{r}
fit_hierarchical <- fit_hierarchical_model(10000, 2, 2000, 2)
print(fit_hierarchical)
```

#### Effective sample size

According to BDA3 Chapter 11.5, it is generally sufficient to have 10 times number of chains effective sample sizes to obtain a reasonable precision from the simulation. Since we're using 7 chains (as can be seen from the output above), we need to make sure that there are at least 7 * 10 = 70 effective sample sizes for each of the parameters.

As can be seen, all parameters have $\hat n_{eff}$ greater than *3650*, which is clearly sufficient given the recommended value of 70.

Besides, according to https://mc-stan.org/users/documentation/case-studies/rstan_workflow.html, "When $n_{eff} / n_{transitions} < 0.001$ the estimators that we use are often biased and can significantly overestimate the true effective sample size.". For our case, the lowest ratio is 3650 / 21000 = 0.174 when taking all post-warmup draws as $n_{transitions}$.

Thus, we can consider our simulation precise enough based on the $\hat n_{eff}$ values.

#### R hat diagnostics

The below interpretation is partially based on the information given in BDA3 chapter 11 and on RStan
documentation (see https://rdrr.io/cran/rstan/man/Rhat.html)

$\hat R$ represents the potential scale reduction factor. $\hat R$ of the values provide convergence diagnostic for - in
this case - 10 chains. It compares “within- and between- chain estimates for model parameters and other
univariate quantities of interest”. If $\hat R$ is not near 1, we need either to coninue our simulations further or to
change the algorithm to be more efficient. How close we want $\hat R$ to be to 1 depends on a problem at hand,
but values smaller than 1.1 or even 1.01 to be safe are generally considered as acceptable

As can be seen from the output above, $\hat R$ for all of the parameters is either 1 or very close to 1. In particular, the values are smaller than a recommended threshold of 1.1 (BDA3 Chapter 11.5).

*We can conclude from this and the previously done ESS diagnostic* that the sequences converged well and we don't need to run the simulations further.

#### HMC -specific convergence diagnostics

The utility function below prints the number of iterations that sanurated the maximum tree depth (set to 15 in this case), and the number of iterations that ended with a divergence.

The utility function uses the code from https://github.com/avehtari/BDA_R_demos/blob/master/demos_rstan/stan_utility.R

```{r}
print_fit_diagnostic(fit_hierarchical, 10)
print_fit_diagnostic(fit_hierarchical, 15)
```

Quoting from https://mc-stan.org/users/documentation/case-studies/rstan_workflow.html, "The dynamic implementation of Hamiltonian Monte Carlo used in Stan has a maximum trajectory length built in to avoid infinite loops that can occur for non-identified models. For sufficiently complex models, however, Stan can saturate this threshold even if the model is identified, which limits the efficacy of the sampler."

As can be seen from the output above, some iterations saturated the tree depth of 10. However, as can be noticed the fitting was run with maximum tree depth of 15. With tree-depth set to 15, there are 0 saturations.

Thus, we can conclude that having max. tree depth set to 15, allows NUTS to not be terminated prematurely and thus the efficientcy of our sampling algorithm is not being restricted.

Besides, we can see that only 2 iterations ended with divergence in the fitting above, which can be safely neglected. Divergences in a simulation might be a indicator of pathological neighbourhoodsof the posterior that the simulated Hamiltonian trajectories are not able to explore sufficiently well (see https://mc-stan.org/users/documentation/case-studies/rstan_workflow.html). 

We thus can conclude that there are no apparent issues with divergences which means that the simulation was able to explore all the important parts of the posterior sufficiently well.

#### PSIS-LOO and LFO

In order to evaluate the model's predictive performance, which is one of the things that matters the most according to the analysis problem section, we're running PSIS-LOO analysis of the model in the code snippet below.

Notice that we've decided to leave first 3 fitted values out of the analysis. Because the model is not fit for the first data point at all (see model description section for details), we thus doing PSIS-LOO only for 5 - 10 datapoints of each age group.

If loo() function is run on the entire dataset without oimtting first several value, $\hat k$ values for those first datapoints tend to be very bad (most of them > 1.0). Besides, the similar technique was used in http://mc-stan.org/loo/articles/loo2-lfo.html. One justification for this could be that we need to have some histoical observations first (at least 4 in our case) to make a decent prediction for the next value.

```{r}
loo_hierarchical <- get_loo_diagnostics(fit_hierarchical)
print(loo_hierarchical)
plot(loo_hierarchical)
```

We can see from the print out above that $\hat k$ values for some of the datapoints are above 0.7. Becasause of this and because PSIS-LOO uses data from the future datapoints, we cannot fully rely on ELPD shown above (the result might be overoptimistic). For these reasons, we'll do a true (as opposed to PSIS approximation) leave-future-out corss validation. The code used in the function below is adapted from http://mc-stan.org/loo/articles/loo2-lfo.html.

```{r}
exact_elpd_hierarchical <- get_lfo_cv_results_hierarchical(10000, 2, 2000, 2)
rbind_print(
  "LFO" = exact_elpd_hierarchical
)
```

Thus, this value can be used later when comparing the models in terms of their predictive performance.

### Graphical posterior predictive checking

Below are the plots of of the true data plotted against 21000 draws from the posterior of the model. The draws from the posterior are summarised using "ribbons" plots.

```{r}
plot_ppc_ribbons(fit_hierarchical, full_grouped_data)
```

We can see from the plots that the draws from the model do not overlap with the actual data in many cases. Moreover, it seems that the model is way too confident with most of the groups resulting in way too narrow "ribbons" in plots above that do not match with the data.

It can also be observed that in many cases predictions for point $y$ are closer to the true value of $y-1$ than to the true value of $y$. This can be explained by the constraints of the selected model (AR(1)) where a predicted value of $y$ highly depends on the true value of the (chronologically) previous datapoint.

To demonstrate further that the model appears to be too confident, we plot the same data (only some age groups, the reset were omitted for brevity) using "ppc_intervals" function (see below).

```{r}
plot_ppc_intervals(fit_hierarchical, full_grouped_data)
```


Finally, in order to demonstrate further how wrong is the model, the error scatter plot is plotted below demonstrating average errors (of y_pred when compared to the actual data) for each datapoint in each age group.

```{r}
y_rep <- as.matrix(fit_hierarchical, pars = "y_rep")
ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep)
```

We can see that for most of the datapoinst, the average error is below 150. Moreover, approximately the same number of data points have negative error as the number of datapoints that have positive average error. It also can be noted that there is moer variation in values of average error for datapoints whose true value is above 1000 with extreme cases being -350 and just under 500.

*From everything discussed above in the seciton, we can conclude that* the model is quite poor in terms of how it fits the data. From the simple visualizaions shown above, we can assume that the main reasons for this is:

1. Uncertainty intervals are not wide enough
2. The fact that data depends mainly on hte previous observation only (because of AR(1)), since data changes quite a bit on some occasions for the year $y$ compared to the year $y-1$.

### Prior sensitivity analysis

Now let's try another sensible but wider prior, while keeping results of the previous fit as our baseline.

```{r}
fit_hierarchical_wide_prior = fit_hierarchical_model(100000, 25, 10000, 25) # wider prior
print(fit_hierarchical_wide_prior)
```

Again, we can see that ESS and $\hat R$ values are good. With smallest ESS still exceeding 3000.

```{r}
print_fit_diagnostic(fit_hierarchical_wide_prior, 15)
```

No iterations saturated the maximum tree depth of 15 and there were only 5 divergences out of 21000, which is good enough for us to proceed with the model.

```{r}
loo_hierarchical_wide_prior <- get_loo_diagnostics(fit_hierarchical_wide_prior)
print(loo_hierarchical_wide_prior)
plot(loo_hierarchical_wide_prior)
```

PSIS-LOO analysis looks similar to the baseline case, where $\hat k$ values for some datapoinst exceeded 0.7. Also estimated ELPD_LOO value is very close to the baseline one.

```{r}
plot_ppc_ribbons(fit_hierarchical_wide_prior, full_grouped_data)
```

```{r}
plot_ppc_intervals(fit_hierarchical_wide_prior, full_grouped_data)
```

Both ribbons and intervals visualizations look similar to the baseline case.

```{r}
# exact_elpd_hierarchical_wide_prior <- get_lfo_cv_results_hierarchical(100000, 25, 10000, 25)
rbind_print(
  "LFO" = exact_elpd_hierarchical_wide_prior
)
```

ELPD_LFO value is almost exaclty the same as in out baseline case (which was -581.87). So we can conclude that there is almost no difference between the models in terms of their predictive performance.


*Now let's try a stronger more narrow prior*

```{r}
fit_hierarchical_narrow_prior = fit_hierarchical_model(2000, 0.5, 10, 0.1)
print(fit_hierarchical_narrow_prior)
```

Again, we can see that ESS and $\hat R$ values are good. With smallest ESS still exceeding 3000.

```{r}
print_fit_diagnostic(fit_hierarchical_narrow_prior, 15)
```

No tree depth saturations and 0 divergences, which is even better compared to the previous cases where we did have negligeble number of divergences.

```{r}
loo_hierarchical_narrow_prior <- get_loo_diagnostics(fit_hierarchical_narrow_prior)
print(loo_hierarchical_narrow_prior)
plot(loo_hierarchical_narrow_prior)
```

PSIS-LOO diagnostics plot looks similar to the previous cases, with estimated ELPD value just slightly worse in the case of the narrow prior.

```{r}
plot_ppc_ribbons(fit_hierarchical_narrow_prior, full_grouped_data)
```

```{r}
plot_ppc_intervals(fit_hierarchical_narrow_prior, full_grouped_data)
```

Similarly to the case of the wider prior, no noticable difference can be observed from the plots above compared to the other priors.

```{r}
exact_elpd_hierarchical_narrow_prior <- get_lfo_cv_results_hierarchical(2000, 0.5, 10, 0.1)
rbind_print(
  "LFO" = exact_elpd_hierarchical_narrow_prior
)
```

ELPD value calculated based on LFO analysis is very close to the other 2 demonstrated before.

The scatter plot below depicts average errors for different datapoints. X axis represents y - y_rep value, while Y axix represents a true value of the datapoint (hence y).

```{r}
y_rep <- as.matrix(fit_hierarchical, pars = "y_rep")
y_rep_wide_prior <- as.matrix(fit_hierarchical_wide_prior, pars = "y_rep")
y_rep_narrow_prior <- as.matrix(fit_hierarchical_narrow_prior, pars = "y_rep")
plot_baseline = ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep)
plot_wide_prior = ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep_wide_prior)
plot_narrow_prior = ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep_narrow_prior)

plot(plot_wide_prior)
plot(plot_baseline)
plot(plot_narrow_prior)
```

Thus, based on the plots above and comparing ELDP values obtained using LFO-CV for each of the 3 different priors we can conclude that the model is not sensitive to a reasonable changes in it's prior and is therefore can be considered as robust and highly dependent on data.

## Model 2

This is the separate AR(1) model. In this model, we asssume that there is no correlation between each age group. As indicated in the hierachial AR(1) model description, Poisson distribution is suitable for modeling the number of each team per year of each age group.

```{stan, output.var="stan_model_ar_separate"}
data {
  int<lower=0> N;
  int<lower=0> K; // number of groups
  int<lower=1,upper=K> x[N]; // group indicator
  int<lower=0> y[N];
  int<lower=0> y0[K];
  real<lower=0> alpha_prior_sd;
  real<lower=0> beta_prior_sd;
  int<lower=0, upper=1> use_lfo_cv;
}
parameters {
  real alpha[K];
  real<lower=0> beta[K];
}
transformed parameters {
  real<lower=0> mu[use_lfo_cv ? (N - K) : N];
  
  for (n in 1:K) {
    mu[n] = alpha[x[n]] + beta[x[n]] * y0[n];
  }
  
  for (n in K+1:(use_lfo_cv ? (N - K) : N)) {
    mu[n] = alpha[x[n]] + beta[x[n]] * y[n-K];
  }
}
model {
  for (n in 1:K) {
    alpha[n] ~ normal(0, alpha_prior_sd); // prior
    beta[n] ~ normal(0, beta_prior_sd);  // prior
  }
  
  for (n in 1:(use_lfo_cv ? (N - K) : N)) {
     y[n] ~ poisson(mu[n]);
  }
}
generated quantities {
  vector[N] log_lik;
  real<lower=0> mu_pred[use_lfo_cv ? K : 0];
  vector[use_lfo_cv ? 0 : N] y_rep;
  
  if (use_lfo_cv) {
    for (n in 1:K) {
      mu_pred[n] = alpha[n] + beta[n] * y[(N - (1 * K)) - K + n];
    }
    
    for (i in 1:(N - (1 * K))) {
      log_lik[i] = poisson_lpmf(y[i] | mu[i]);
    }
    
    for (i in 1:K) {
      log_lik[(N - (1 * K)) + i] = poisson_lpmf(y[(N - (1 * K)) + i] | mu_pred[i]);
    }
  } else {
    for (i in 1:N) {
      y_rep[i] = poisson_rng(mu[i]);
      log_lik[i] = poisson_lpmf(y[i] | mu[i]);
    }
  }
}
```


### Prior predictive checking

```{r}
check_prior_separate(full_grouped_data, 0, 10000, 0, 2)
```

```{r}
check_prior_separate(full_grouped_data, 0, 10000, 1, 1)
```

```{r}
check_prior_separate(full_grouped_data, 0, 10000, 0, 25)
```

```{r}
check_prior_separate(full_grouped_data, 0, 30000, 0, 2)
```


```{r}
fit_separate = fit_separate_model(10000, 2)
print(fit_separate)
```

All of the $\hat R$ values look good and ESS values are all well above 10000, which is more than enough given tha we only have 7 chains (7 * 10 = 70).

```{r}
print_fit_diagnostic(fit_separate, 10)
```

HCM -specific convergence diagnostics looks very good too.

*PSIS-LOO*

```{r}
loo_separate <- get_loo_diagnostics(fit_separate)
print(loo_separate)
plot(loo_separate)
```



*LFO-LOO*

```{r}
exact_elpd_separate <- get_lfo_cv_results_separate(10000, 2)
rbind_print(
  "LFO" = exact_elpd_separate
)
```

### Graphical posterior predictive checking

```{r}
plot_ppc_ribbons(fit_separate, full_grouped_data)
```

```{r}
plot_ppc_intervals(fit_separate, full_grouped_data)
```

```{r}
y_rep <- as.matrix(fit_separate, pars = "y_rep")
ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep)
```

### Prior sensitivity analysis

Now let's try another sensible but a little wider prior, while keeping results of the previous fit as our base-line.
```{r}
# fit_separate_wide_prior = fit_separate_model(10000, 25)
print(fit_separate_wide_prior)
```

Again, ESS and $\hat R$ both look good

```{r}
print_fit_diagnostic(fit_separate_wide_prior, 10)
```

No tree depth saturations and only 2 divergences per 21000 iterations.

```{r}
loo_separate_wide_prior <- get_loo_diagnostics(fit_separate_wide_prior)
print(loo_separate_wide_prior)
plot(loo_separate_wide_prior)
```


```{r}
plot_ppc_ribbons(fit_separate_wide_prior, full_grouped_data)
```

```{r}
exact_elpd_separate_wide_prior <- get_lfo_cv_results_separate(10000, 25)
rbind_print(
  "LFO" = exact_elpd_separate_wide_prior
)
```

Now let's try a stronger more narrow prior

```{r}
fit_separate_narrow_prior = fit_separate_model(2000, 0.5)
print(fit_separate_narrow_prior)
```

Similarly ESS and $\hat R$ are very good.

```{r}
print_fit_diagnostic(fit_separate_narrow_prior, 10)
```

Again, no max. tree depth saturation and only 7 divergences (which is 0.03% of all 21000 iterations). Thus, we can conclude that HMC -specifica convergence diagnostics looks good too.

```{r}
loo_separate_narrow_prior <- get_loo_diagnostics(fit_separate_narrow_prior)
print(loo_separate_narrow_prior)
plot(loo_separate_narrow_prior)
```

```{r}
exact_elpd_separate_narrow_prior <- get_lfo_cv_results_separate(2000, 0.5)
rbind_print(
  "LFO" = exact_elpd_separate_narrow_prior
)
```

Rather surprisingly the ELPD result is better for the separate model when a stricter prior is used. Although the differrence is not significant, the last model has the same ELPD value as the hierarchical models presented previously.

```{r}
y_rep <- as.matrix(fit_separate, pars = "y_rep")
y_rep_wide_prior <- as.matrix(fit_separate_wide_prior, pars = "y_rep")
y_rep_narrow_prior <- as.matrix(fit_separate_narrow_prior, pars = "y_rep")
plot_baseline = ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep)
plot_wide_prior = ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep_wide_prior)
plot_narrow_prior = ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), y_rep_narrow_prior)

plot(plot_wide_prior)
plot(plot_baseline)
plot(plot_narrow_prior)
```

## Model comparison

Here is a comparison for hierarchical and separate model respectively

```{r}
ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), as.matrix(fit_hierarchical, pars = "y_rep"))
ppc_error_scatter_avg(c(full_grouped_data[, (2:10)]), as.matrix(fit_separate, pars = "y_rep"))
```

As can be seen the difference between the 2 models is rather small one.

```{r}
comparison_results = matrix(
  c(
    loo_hierarchical$estimates[1:2], # PSIS-LOO -based elpd and p_eff values
    exact_elpd_hierarchical, # LFO-based elpd
    loo_hierarchical_wide_prior$estimates[1:2], # PSIS-LOO -based elpd and p_eff values
    exact_elpd_hierarchical_wide_prior, # LFO-based elpd
    loo_hierarchical_narrow_prior$estimates[1:2], # PSIS-LOO -based elpd and p_eff values
    exact_elpd_hierarchical_narrow_prior, # LFO-based elpd
    loo_separate$estimates[1:2], # PSIS-LOO -based elpd and p_eff values
    exact_elpd_separate, # LFO-based elpd
    loo_separate_wide_prior$estimates[1:2], # PSIS-LOO -based elpd and p_eff values
    exact_elpd_separate_wide_prior, # LFO-based elpds
    loo_separate_narrow_prior$estimates[1:2], # PSIS-LOO -based elpd and p_eff values
    exact_elpd_separate_narrow_prior # LFO-based elpd
  ),
  ncol = 6,
  dimnames = list(
    c("elpd_loo", "p_loo", "elpd_lfo"),
    c("hierarchical",
      "hierarchical (wide prior)",
      "hierarchical (narrow prior)",
      "separate",
      "separate (wide prior)",
      "separate (narrow prior)")
  )
)
comparison_results
```

```{r}
kable(comparison_results)
```

Thus, we can see that while both of the models (hierarchical and separate are far from being perfect when trying to fit the data), hierarchical is a bit better (according ELPD value obtained as a result of LFO-CV) than the separate model. This is true for the "original" and "wider" priors. However, intereestingly, one can see that the separate model just slightly outperforms the hierarchical one if a stringer prior is used for it.



# Problems encountered and potential improvements


# Conclusion

// a clear conclusion here
